{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsFqqI3E8Xn4"
      },
      "source": [
        "## Chatbots With LangGraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgKIrwXJAjgA",
        "outputId": "7eec3700-d2d7-4ca3-bc2c-875c9cae1dd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "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\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "groq_api_key=os.getenv('groq_api_key')\n",
        "LANGSMITH_API_KEY=os.getenv('LANGSMITH_API_KEY')\n",
        "LANGCHAIN_API_KEY=os.getenv('LANGCHAIN_API_KEY')\n",
        "print(LANGSMITH_API_KEY)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YFpKBPoZBQAa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = LANGCHAIN_API_KEY\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"]=\"CourseLanggraph\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XJZl9uX-CFo7"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9vOWf8yCJ4W",
        "outputId": "91931387-650d-4d7a-95de-31735c66ce27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001C67D8C9D90>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001C67D8DF310>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"Gemma2-9b-It\")\n",
        "llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDc7RviFCVKB"
      },
      "source": [
        "## Start Building Chatbot Using Langgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bPcSBey6CUeF"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph,START,END\n",
        "from langgraph.graph.message import add_messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dVzn-8rrC6-7"
      },
      "outputs": [],
      "source": [
        "class State(TypedDict):\n",
        "  # Messages have the type \"list\". The `add_messages` function\n",
        "    # in the annotation defines how this state key should be updated\n",
        "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
        "  messages:Annotated[list,add_messages]\n",
        "\n",
        "graph_builder=StateGraph(State)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgjwX6BQDcfk",
        "outputId": "10f24dc5-6242-454b-b61c-cfecf260ae22"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x1c67d92e400>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_builder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oOF74t5GDeJO"
      },
      "outputs": [],
      "source": [
        "def chatbot(state:State):\n",
        "  return {\"messages\":llm.invoke(state['messages'])}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-m7NDe74EPGW"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x1c67d92e400>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_builder.add_node(\"chatbot\",chatbot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKFbmbqcEVIq",
        "outputId": "9f91e0af-022e-4485-8151-db69c5cc3ccb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x1c67d92e400>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_builder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3jBWX44IEWy-"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x1c67d92e400>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_builder.add_edge(START,\"chatbot\")\n",
        "graph_builder.add_edge(\"chatbot\",END)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PN4O63shEtyZ"
      },
      "outputs": [],
      "source": [
        "graph=graph_builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "_PPER3gfEygm",
        "outputId": "fbe87a93-4e15-47e7-9cc3-5cebe43ce64f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "try:\n",
        "  display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except Exception:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDvjUFmxFCwU",
        "outputId": "c77c0d85-b6f5-4dee-81c5-5c728019aaed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_values([{'messages': AIMessage(content='Hello! ðŸ‘‹  How can I help you today?\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 11, 'total_tokens': 25, 'completion_time': 0.025454545, 'prompt_time': 0.001990066, 'queue_time': 0.24931652399999998, 'total_time': 0.027444611}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--197c01ee-719e-4825-8acb-f0a7aacd9761-0', usage_metadata={'input_tokens': 11, 'output_tokens': 14, 'total_tokens': 25})}])\n",
            "content='Hello! ðŸ‘‹  How can I help you today?\\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 11, 'total_tokens': 25, 'completion_time': 0.025454545, 'prompt_time': 0.001990066, 'queue_time': 0.24931652399999998, 'total_time': 0.027444611}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run--197c01ee-719e-4825-8acb-f0a7aacd9761-0' usage_metadata={'input_tokens': 11, 'output_tokens': 14, 'total_tokens': 25}\n",
            "Assistant: Hello! ðŸ‘‹  How can I help you today?\n",
            "\n",
            "dict_values([{'messages': AIMessage(content='LangChain is an open-source framework designed to simplify the development of applications powered by large language models (LLMs). Think of it as a toolbox filled with components you can use to build sophisticated AI applications.\\n\\nHere\\'s a breakdown of what makes LangChain special:\\n\\n**Key Features:**\\n\\n* **Modular Design:** LangChain breaks down complex tasks into smaller, manageable modules. This allows developers to easily combine different components to create customized workflows.\\n\\n* **Chain Creation:** At its core, LangChain enables you to create \"chains\" â€“ sequences of connected LLMs and other tools. This allows for more complex interactions and reasoning. Imagine a chain that first summarizes a document, then answers questions about it, and finally generates a creative text based on the information.\\n\\n* **Memory Management:** LangChain provides mechanisms to store and retrieve information across multiple interactions with an LLM. This enables your applications to have context and remember past exchanges, leading to more natural and engaging conversations.\\n\\n* **Agent Capabilities:** LangChain empowers you to build \"agents\" â€“ autonomous AI entities that can interact with the world. These agents can access external tools, process information, and make decisions based on their goals.\\n\\n* **Integration with LLMs:** LangChain seamlessly integrates with various LLMs like OpenAI\\'s GPT models, Google\\'s PaLM, and others. This gives you flexibility in choosing the best model for your needs.\\n* **Extensible Ecosystem:** LangChain has a growing community and a vibrant ecosystem of plugins and integrations. This means you can easily add new functionalities and tailor the framework to your specific use cases.\\n\\n**Use Cases:**\\n\\nLangChain\\'s versatility makes it suitable for a wide range of applications:\\n\\n* **Chatbots and Conversational AI:** Build more intelligent and context-aware chatbots.\\n* **Question Answering Systems:** Create systems that can answer complex questions based on given documents or knowledge bases.\\n* **Text Summarization and Analysis:** Summarize large amounts of text or extract key insights.\\n* **Code Generation and Assistance:** Generate code snippets or assist developers in writing code.\\n* **Data Augmentation and Generation:** Generate synthetic data for training machine learning models.\\n\\n**Getting Started:**\\n\\nLangChain is relatively easy to learn and use.\\n\\n* **Documentation:** The official LangChain documentation is comprehensive and well-written: [https://python.langchain.com/](https://python.langchain.com/)\\n* **Examples:** The repository includes numerous examples to get you started quickly.\\n\\n\\nLet me know if you have any more questions about LangChain or want to explore specific use cases in more detail!\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 543, 'prompt_tokens': 14, 'total_tokens': 557, 'completion_time': 0.987272727, 'prompt_time': 0.002016856, 'queue_time': 0.24609110399999998, 'total_time': 0.989289583}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--344f57a7-7991-496e-88e5-8c3cb4a7a660-0', usage_metadata={'input_tokens': 14, 'output_tokens': 543, 'total_tokens': 557})}])\n",
            "content='LangChain is an open-source framework designed to simplify the development of applications powered by large language models (LLMs). Think of it as a toolbox filled with components you can use to build sophisticated AI applications.\\n\\nHere\\'s a breakdown of what makes LangChain special:\\n\\n**Key Features:**\\n\\n* **Modular Design:** LangChain breaks down complex tasks into smaller, manageable modules. This allows developers to easily combine different components to create customized workflows.\\n\\n* **Chain Creation:** At its core, LangChain enables you to create \"chains\" â€“ sequences of connected LLMs and other tools. This allows for more complex interactions and reasoning. Imagine a chain that first summarizes a document, then answers questions about it, and finally generates a creative text based on the information.\\n\\n* **Memory Management:** LangChain provides mechanisms to store and retrieve information across multiple interactions with an LLM. This enables your applications to have context and remember past exchanges, leading to more natural and engaging conversations.\\n\\n* **Agent Capabilities:** LangChain empowers you to build \"agents\" â€“ autonomous AI entities that can interact with the world. These agents can access external tools, process information, and make decisions based on their goals.\\n\\n* **Integration with LLMs:** LangChain seamlessly integrates with various LLMs like OpenAI\\'s GPT models, Google\\'s PaLM, and others. This gives you flexibility in choosing the best model for your needs.\\n* **Extensible Ecosystem:** LangChain has a growing community and a vibrant ecosystem of plugins and integrations. This means you can easily add new functionalities and tailor the framework to your specific use cases.\\n\\n**Use Cases:**\\n\\nLangChain\\'s versatility makes it suitable for a wide range of applications:\\n\\n* **Chatbots and Conversational AI:** Build more intelligent and context-aware chatbots.\\n* **Question Answering Systems:** Create systems that can answer complex questions based on given documents or knowledge bases.\\n* **Text Summarization and Analysis:** Summarize large amounts of text or extract key insights.\\n* **Code Generation and Assistance:** Generate code snippets or assist developers in writing code.\\n* **Data Augmentation and Generation:** Generate synthetic data for training machine learning models.\\n\\n**Getting Started:**\\n\\nLangChain is relatively easy to learn and use.\\n\\n* **Documentation:** The official LangChain documentation is comprehensive and well-written: [https://python.langchain.com/](https://python.langchain.com/)\\n* **Examples:** The repository includes numerous examples to get you started quickly.\\n\\n\\nLet me know if you have any more questions about LangChain or want to explore specific use cases in more detail!\\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 543, 'prompt_tokens': 14, 'total_tokens': 557, 'completion_time': 0.987272727, 'prompt_time': 0.002016856, 'queue_time': 0.24609110399999998, 'total_time': 0.989289583}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run--344f57a7-7991-496e-88e5-8c3cb4a7a660-0' usage_metadata={'input_tokens': 14, 'output_tokens': 543, 'total_tokens': 557}\n",
            "Assistant: LangChain is an open-source framework designed to simplify the development of applications powered by large language models (LLMs). Think of it as a toolbox filled with components you can use to build sophisticated AI applications.\n",
            "\n",
            "Here's a breakdown of what makes LangChain special:\n",
            "\n",
            "**Key Features:**\n",
            "\n",
            "* **Modular Design:** LangChain breaks down complex tasks into smaller, manageable modules. This allows developers to easily combine different components to create customized workflows.\n",
            "\n",
            "* **Chain Creation:** At its core, LangChain enables you to create \"chains\" â€“ sequences of connected LLMs and other tools. This allows for more complex interactions and reasoning. Imagine a chain that first summarizes a document, then answers questions about it, and finally generates a creative text based on the information.\n",
            "\n",
            "* **Memory Management:** LangChain provides mechanisms to store and retrieve information across multiple interactions with an LLM. This enables your applications to have context and remember past exchanges, leading to more natural and engaging conversations.\n",
            "\n",
            "* **Agent Capabilities:** LangChain empowers you to build \"agents\" â€“ autonomous AI entities that can interact with the world. These agents can access external tools, process information, and make decisions based on their goals.\n",
            "\n",
            "* **Integration with LLMs:** LangChain seamlessly integrates with various LLMs like OpenAI's GPT models, Google's PaLM, and others. This gives you flexibility in choosing the best model for your needs.\n",
            "* **Extensible Ecosystem:** LangChain has a growing community and a vibrant ecosystem of plugins and integrations. This means you can easily add new functionalities and tailor the framework to your specific use cases.\n",
            "\n",
            "**Use Cases:**\n",
            "\n",
            "LangChain's versatility makes it suitable for a wide range of applications:\n",
            "\n",
            "* **Chatbots and Conversational AI:** Build more intelligent and context-aware chatbots.\n",
            "* **Question Answering Systems:** Create systems that can answer complex questions based on given documents or knowledge bases.\n",
            "* **Text Summarization and Analysis:** Summarize large amounts of text or extract key insights.\n",
            "* **Code Generation and Assistance:** Generate code snippets or assist developers in writing code.\n",
            "* **Data Augmentation and Generation:** Generate synthetic data for training machine learning models.\n",
            "\n",
            "**Getting Started:**\n",
            "\n",
            "LangChain is relatively easy to learn and use.\n",
            "\n",
            "* **Documentation:** The official LangChain documentation is comprehensive and well-written: [https://python.langchain.com/](https://python.langchain.com/)\n",
            "* **Examples:** The repository includes numerous examples to get you started quickly.\n",
            "\n",
            "\n",
            "Let me know if you have any more questions about LangChain or want to explore specific use cases in more detail!\n",
            "\n",
            "Good Bye\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "  user_input=input(\"User: \")\n",
        "  if user_input.lower() in [\"quit\",\"q\"]:\n",
        "    print(\"Good Bye\")\n",
        "    break\n",
        "  for event in graph.stream({'messages':(\"user\",user_input)}):\n",
        "    print(event.values())\n",
        "    for value in event.values():\n",
        "      print(value['messages'])\n",
        "      print(\"Assistant:\",value[\"messages\"].content)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
